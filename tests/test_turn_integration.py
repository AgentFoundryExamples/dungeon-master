# Copyright 2025 John Brosnihan
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Integration tests for the /turn endpoint orchestration."""

import pytest
from unittest.mock import AsyncMock, patch
import os


@pytest.mark.asyncio
async def test_turn_endpoint_full_flow_stub_mode(client):
    """Test the full turn endpoint flow in stub mode.
    
    Verifies:
    - GET /context called once with correct params (recent_n=20, include_pois=false)
    - POST /narrative called once with user_action and narrative
    - Returns mocked LLM narrative
    """
    from httpx import Response
    from unittest.mock import MagicMock
    
    # Mock journey-log context response
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Aria", "race": "Elf", "class": "Ranger"},
            "status": "Healthy",
            "location": {"id": "origin:nexus", "display_name": "The Nexus"}
        },
        "quest": None,
        "combat": {"active": False, "state": None},
        "narrative": {
            "recent_turns": []
        }
    }
    mock_context_response.raise_for_status = MagicMock()
    
    # Mock journey-log persist response
    mock_persist_response = MagicMock(spec=Response)
    mock_persist_response.raise_for_status = MagicMock()
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post:
        
        mock_get.return_value = mock_context_response
        mock_post.return_value = mock_persist_response
        
        # Make request to turn endpoint
        character_id = "550e8400-e29b-41d4-a716-446655440000"
        user_action = "I search the ancient temple"
        response = client.post(
            "/turn",
            headers={"X-User-Id": "test-user-123"},
            json={
                "character_id": character_id,
                "user_action": user_action
            }
        )
        
        # Verify response
        assert response.status_code == 200
        data = response.json()
        assert "narrative" in data
        assert len(data["narrative"]) > 0
        # In stub mode, should contain [STUB MODE]
        assert "[STUB MODE]" in data["narrative"]
        
        # Verify GET context was called once with correct parameters
        mock_get.assert_called_once()
        get_call_args = mock_get.call_args
        get_url = get_call_args[0][0]
        # Verify exact URL path construction
        assert get_url == f"http://localhost:8000/characters/{character_id}/context"
        assert get_call_args[1]["params"]["recent_n"] == 20
        assert get_call_args[1]["params"]["include_pois"] is False
        
        # Verify POST narrative was called once with user_action and narrative
        mock_post.assert_called_once()
        post_call_args = mock_post.call_args
        post_url = post_call_args[0][0]
        # Verify exact URL path construction
        assert post_url == f"http://localhost:8000/characters/{character_id}/narrative"
        post_data = post_call_args[1]["json"]
        assert post_data["user_action"] == user_action
        assert "ai_response" in post_data
        assert len(post_data["ai_response"]) > 0
        
        # Verify X-User-Id header was passed to both calls
        assert get_call_args[1]["headers"]["X-User-Id"] == "test-user-123"
        assert post_call_args[1]["headers"]["X-User-Id"] == "test-user-123"


@pytest.mark.asyncio
async def test_turn_endpoint_character_not_found(client):
    """Test turn endpoint when character not found."""
    from httpx import HTTPStatusError, Request, Response
    from unittest.mock import MagicMock
    
    # Mock 404 response from journey-log
    mock_request = MagicMock(spec=Request)
    mock_response = MagicMock(spec=Response)
    mock_response.status_code = 404
    mock_response.text = "Character not found"
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get:
        mock_get.side_effect = HTTPStatusError(
            "404 Not Found",
            request=mock_request,
            response=mock_response
        )
        
        response = client.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I search"
            }
        )
        
        assert response.status_code == 404
        data = response.json()
        # Check structured error response
        assert "error" in data["detail"]
        assert data["detail"]["error"]["type"] == "character_not_found"
        assert "not found" in data["detail"]["error"]["message"].lower()


@pytest.mark.asyncio
async def test_turn_endpoint_journey_log_timeout(client):
    """Test turn endpoint when journey-log times out."""
    from httpx import TimeoutException
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get:
        mock_get.side_effect = TimeoutException("Timeout")
        
        response = client.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I search"
            }
        )
        
        assert response.status_code == 504  # Gateway timeout
        data = response.json()
        # Check structured error response
        assert "error" in data["detail"]
        assert data["detail"]["error"]["type"] == "journey_log_timeout"
        assert "timed out" in data["detail"]["error"]["message"].lower()


@pytest.mark.asyncio
async def test_turn_endpoint_with_user_id_header(client):
    """Test turn endpoint with X-User-Id header for request correlation."""
    from httpx import Response
    from unittest.mock import MagicMock
    
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Aria", "race": "Elf", "class": "Ranger"},
            "status": "Healthy",
            "location": {"id": "origin:nexus", "display_name": "The Nexus"}
        },
        "quest": None,
        "combat": {"active": False, "state": None},
        "narrative": {"recent_turns": []}
    }
    mock_context_response.raise_for_status = MagicMock()
    
    mock_persist_response = MagicMock(spec=Response)
    mock_persist_response.raise_for_status = MagicMock()
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post:
        
        mock_get.return_value = mock_context_response
        mock_post.return_value = mock_persist_response
        
        response = client.post(
            "/turn",
            headers={"X-User-Id": "test-user-123"},
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I search"
            }
        )
        
        assert response.status_code == 200
        
        # Verify user_id was passed to journey-log calls as X-User-Id
        mock_get.assert_called_once()
        get_call_kwargs = mock_get.call_args[1]
        assert get_call_kwargs["headers"].get("X-User-Id") == "test-user-123"


@pytest.mark.asyncio
async def test_turn_endpoint_persist_failure_returns_error(client):
    """Test that turn endpoint returns success with failure in summary when persistence fails.
    
    New behavior: Narrative persistence failure doesn't block the response.
    Instead, returns 200 with narrative_persisted=False in subsystem_summary.
    """
    from httpx import Response, HTTPStatusError, Request
    from unittest.mock import MagicMock
    
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Aria", "race": "Elf", "class": "Ranger"},
            "status": "Healthy",
            "location": {"id": "origin:nexus", "display_name": "The Nexus"}
        },
        "quest": None,
        "combat": {"active": False, "state": None},
        "narrative": {"recent_turns": []}
    }
    mock_context_response.raise_for_status = MagicMock()
    
    # Mock persist failure
    mock_request = MagicMock(spec=Request)
    mock_error_response = MagicMock(spec=Response)
    mock_error_response.status_code = 500
    mock_error_response.text = "Internal server error"
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post:
        
        mock_get.return_value = mock_context_response
        mock_post.side_effect = HTTPStatusError(
            "500 Internal Server Error",
            request=mock_request,
            response=mock_error_response
        )
        
        response = client.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I search"
            }
        )
        
        # New behavior: returns 200 with failure in summary
        assert response.status_code == 200
        data = response.json()
        
        # Check that narrative is present
        assert "narrative" in data
        assert len(data["narrative"]) > 0
        
        # Check subsystem_summary shows persistence failure
        assert "subsystem_summary" in data
        summary = data["subsystem_summary"]
        assert summary["narrative_persisted"] is False
        assert summary["narrative_error"] is not None
        assert "journey-log" in summary["narrative_error"].lower() or "500" in summary["narrative_error"]

@pytest.mark.asyncio
async def test_turn_endpoint_llm_failure_skips_narrative_write(client):
    """Test that LLM failure skips narrative write and returns 5xx error.
    
    Verifies:
    - When LLM fails, POST /narrative is NOT called
    - Returns structured 5xx error response
    """
    from httpx import Response
    from unittest.mock import MagicMock
    
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Aria", "race": "Elf", "class": "Ranger"},
            "status": "Healthy",
            "location": {"id": "origin:nexus", "display_name": "The Nexus"}
        },
        "quest": None,
        "combat": {"active": False, "state": None},
        "narrative": {"recent_turns": []}
    }
    mock_context_response.raise_for_status = MagicMock()
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post:
        
        mock_get.return_value = mock_context_response
        
        # Mock LLM client to raise an error
        from app.services.llm_client import LLMClient, LLMResponseError
        with patch.object(LLMClient, 'generate_narrative', new_callable=AsyncMock) as mock_llm:
            mock_llm.side_effect = LLMResponseError("Invalid response from LLM")
            
            response = client.post(
                "/turn",
                json={
                    "character_id": "550e8400-e29b-41d4-a716-446655440000",
                    "user_action": "I search"
                }
            )
            
            # Should return 502 error when LLM fails
            assert response.status_code == 502
            data = response.json()
            
            # Check structured error response
            assert "error" in data["detail"]
            assert data["detail"]["error"]["type"] == "llm_response_error"
            assert "llm" in data["detail"]["error"]["message"].lower()
            assert data["detail"]["error"]["request_id"] is not None
            
            # Verify GET context was called (should succeed before LLM failure)
            mock_get.assert_called_once()
            
            # Verify POST /narrative was NOT called (narrative write skipped)
            mock_post.assert_not_called()


@pytest.mark.asyncio
async def test_turn_endpoint_journey_log_error_no_llm_call(client):
    """Test that journey-log error prevents LLM call.
    
    Verifies:
    - When journey-log fails, LLM is NOT called
    - Returns structured 5xx error response
    """
    from httpx import HTTPStatusError, Request, Response
    from unittest.mock import MagicMock
    
    # Mock 502 response from journey-log
    mock_request = MagicMock(spec=Request)
    mock_response = MagicMock(spec=Response)
    mock_response.status_code = 502
    mock_response.text = "Bad Gateway"
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get:
        mock_get.side_effect = HTTPStatusError(
            "502 Bad Gateway",
            request=mock_request,
            response=mock_response
        )
        
        # Mock LLM client to track if it's called
        from app.services.llm_client import LLMClient
        with patch.object(LLMClient, 'generate_narrative', new_callable=AsyncMock) as mock_llm:
            response = client.post(
                "/turn",
                json={
                    "character_id": "550e8400-e29b-41d4-a716-446655440000",
                    "user_action": "I search"
                }
            )
            
            # Should return 502 error
            assert response.status_code == 502
            data = response.json()
            
            # Check structured error response
            assert "error" in data["detail"]
            assert data["detail"]["error"]["type"] == "journey_log_error"
            assert "journey-log" in data["detail"]["error"]["message"].lower()
            
            # Verify LLM was NOT called
            mock_llm.assert_not_called()


@pytest.mark.asyncio
async def test_turn_endpoint_optional_context_fields(client):
    """Test turn endpoint handles optional context fields (quest/combat absent).
    
    Verifies:
    - Endpoint works when quest and combat_state are None/absent
    - No serialization errors occur
    - Returns successful narrative
    """
    from httpx import Response
    from unittest.mock import MagicMock
    
    # Mock journey-log context response with minimal fields
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Bob", "race": "Human", "class": "Fighter"},
            "status": "Healthy",
            "location": {"id": "town:square", "display_name": "Town Square"}
        },
        # quest is absent (not even None)
        "combat": {"active": False, "state": None},
        "narrative": {
            "recent_turns": []
        }
    }
    mock_context_response.raise_for_status = MagicMock()
    
    mock_persist_response = MagicMock(spec=Response)
    mock_persist_response.raise_for_status = MagicMock()
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post:
        
        mock_get.return_value = mock_context_response
        mock_post.return_value = mock_persist_response
        
        response = client.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I look around"
            }
        )
        
        # Should succeed despite missing optional fields
        assert response.status_code == 200
        data = response.json()
        assert "narrative" in data
        assert len(data["narrative"]) > 0


@pytest.mark.asyncio
async def test_turn_endpoint_metrics_logging_no_errors(client):
    """Test that metrics and logging hooks don't throw errors.
    
    Verifies:
    - Turn endpoint completes successfully with metrics enabled
    - No exceptions from metrics collection
    - No exceptions from logging
    """
    from httpx import Response
    from unittest.mock import MagicMock
    
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Test", "race": "Elf", "class": "Mage"},
            "status": "Healthy",
            "location": {"id": "forest:clearing", "display_name": "Forest Clearing"}
        },
        "quest": None,
        "combat": {"active": False, "state": None},
        "narrative": {"recent_turns": []}
    }
    mock_context_response.raise_for_status = MagicMock()
    
    mock_persist_response = MagicMock(spec=Response)
    mock_persist_response.raise_for_status = MagicMock()
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post, \
         patch.dict(os.environ, {"ENABLE_METRICS": "true"}):
        
        mock_get.return_value = mock_context_response
        mock_post.return_value = mock_persist_response
        
        # This should complete without raising exceptions from metrics/logging
        response = client.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I cast a spell"
            }
        )
        
        # Verify successful completion
        assert response.status_code == 200
        data = response.json()
        assert "narrative" in data


@pytest.mark.asyncio
async def test_turn_endpoint_policy_decisions_logged(client, caplog):
    """Test that policy decisions are logged correctly each turn."""
    from httpx import Response
    from unittest.mock import MagicMock
    import logging
    
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Aria", "race": "Elf", "class": "Ranger"},
            "status": "Healthy",
            "location": {"id": "origin:nexus", "display_name": "The Nexus"}
        },
        "quest": None,
        "combat": {"active": False, "state": None},
        "narrative": {"recent_turns": []}
    }
    mock_context_response.raise_for_status = MagicMock()
    
    mock_persist_response = MagicMock(spec=Response)
    mock_persist_response.raise_for_status = MagicMock()
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post, \
         caplog.at_level(logging.INFO):
        
        mock_get.return_value = mock_context_response
        mock_post.return_value = mock_persist_response
        
        response = client.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I explore the area"
            }
        )
        
        assert response.status_code == 200
        
        # Verify policy decisions were logged
        log_messages = [record.message for record in caplog.records]
        policy_logs = [msg for msg in log_messages if "Policy decisions evaluated" in msg]
        
        # Should have logged policy decisions
        assert len(policy_logs) >= 1, "Expected policy decision log message"
        
        # Check that log contains decision info with specific fields
        policy_log_text = " ".join(policy_logs)
        assert "quest_eligible" in policy_log_text, "Policy log should contain quest_eligible field"
        assert "poi_eligible" in policy_log_text, "Policy log should contain poi_eligible field"


@pytest.mark.asyncio
async def test_turn_endpoint_policy_decisions_not_in_response(client):
    """Test that policy decisions don't leak into player-facing response."""
    from httpx import Response
    from unittest.mock import MagicMock
    
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Aria", "race": "Elf", "class": "Ranger"},
            "status": "Healthy",
            "location": {"id": "origin:nexus", "display_name": "The Nexus"}
        },
        "quest": None,
        "combat": {"active": False, "state": None},
        "narrative": {"recent_turns": []}
    }
    mock_context_response.raise_for_status = MagicMock()
    
    mock_persist_response = MagicMock(spec=Response)
    mock_persist_response.raise_for_status = MagicMock()
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post:
        
        mock_get.return_value = mock_context_response
        mock_post.return_value = mock_persist_response
        
        response = client.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I search the room"
            }
        )
        
        assert response.status_code == 200
        data = response.json()
        
        # Response should only have narrative and optionally intents
        assert "narrative" in data
        # Should NOT have policy decision fields in response
        assert "quest_trigger_decision" not in data
        assert "poi_trigger_decision" not in data
        assert "policy_hints" not in data
        assert "quest_eligible" not in data
        assert "poi_eligible" not in data


@pytest.mark.asyncio
async def test_turn_endpoint_failed_quest_roll_blocks_propagation(client_with_failed_quest_roll):
    """Test that failed quest roll prevents quest suggestion from being persisted.
    
    Verifies that even if LLM suggests a quest, the failed policy roll
    prevents the quest from being written to journey-log.
    """
    from httpx import Response
    from unittest.mock import MagicMock, AsyncMock, patch
    
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Test", "race": "Human", "class": "Fighter"},
            "status": "Healthy",
            "location": {"id": "town:square", "display_name": "Town Square"}
        },
        "quest": None,
        "combat": {"active": False, "state": None},
        "narrative": {"recent_turns": []}
    }
    mock_context_response.raise_for_status = MagicMock()
    
    mock_persist_response = MagicMock(spec=Response)
    mock_persist_response.raise_for_status = MagicMock()
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post:
        
        mock_get.return_value = mock_context_response
        mock_post.return_value = mock_persist_response
        
        response = client_with_failed_quest_roll.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I ask the innkeeper for a quest"
            }
        )
        
        assert response.status_code == 200
        data = response.json()
        
        # Narrative should be present
        assert "narrative" in data
        
        # Verify narrative was persisted
        mock_post.assert_called_once()
        
        # Check the payload sent to journey-log to verify no quest data
        post_call_args = mock_post.call_args
        if post_call_args:
            post_data = post_call_args[1].get("json", {})
            # In stub mode, LLM unlikely to suggest quest anyway, but verify
            # that the response structure doesn't include quest-related fields
            # beyond what's in the narrative text
            assert "ai_response" in post_data or "narrative" in post_data


@pytest.mark.asyncio
async def test_turn_endpoint_failed_poi_roll_blocks_propagation(client_with_failed_poi_roll):
    """Test that failed POI roll prevents POI suggestion from being persisted.
    
    Verifies that even if LLM suggests a POI, the failed policy roll
    prevents the POI from being written to journey-log.
    """
    from httpx import Response
    from unittest.mock import MagicMock, AsyncMock, patch
    
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Test", "race": "Elf", "class": "Rogue"},
            "status": "Healthy",
            "location": {"id": "forest:path", "display_name": "Forest Path"}
        },
        "quest": None,
        "combat": {"active": False, "state": None},
        "narrative": {"recent_turns": []}
    }
    mock_context_response.raise_for_status = MagicMock()
    
    mock_persist_response = MagicMock(spec=Response)
    mock_persist_response.raise_for_status = MagicMock()
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post:
        
        mock_get.return_value = mock_context_response
        mock_post.return_value = mock_persist_response
        
        response = client_with_failed_poi_roll.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I explore the forest"
            }
        )
        
        assert response.status_code == 200
        data = response.json()
        
        # Narrative should be present
        assert "narrative" in data
        
        # Verify narrative was persisted
        mock_post.assert_called_once()
        
        # Check the payload sent to journey-log to verify no POI data
        post_call_args = mock_post.call_args
        if post_call_args:
            post_data = post_call_args[1].get("json", {})
            # Verify that the response structure doesn't include POI-related fields
            # beyond what's in the narrative text
            assert "ai_response" in post_data or "narrative" in post_data


@pytest.mark.asyncio
async def test_turn_endpoint_with_deterministic_seed(client_with_deterministic_seed):
    """Test turn endpoint with deterministic seed produces consistent results."""
    from httpx import Response
    from unittest.mock import MagicMock, AsyncMock, patch
    
    mock_context_response = MagicMock(spec=Response)
    mock_context_response.json.return_value = {
        "character_id": "550e8400-e29b-41d4-a716-446655440000",
        "player_state": {
            "identity": {"name": "Test", "race": "Dwarf", "class": "Warrior"},
            "status": "Healthy",
            "location": {"id": "mountain:peak", "display_name": "Mountain Peak"}
        },
        "quest": None,
        "combat": {"active": False, "state": None},
        "narrative": {"recent_turns": []}
    }
    mock_context_response.raise_for_status = MagicMock()
    
    mock_persist_response = MagicMock(spec=Response)
    mock_persist_response.raise_for_status = MagicMock()
    
    with patch('httpx.AsyncClient.get', new_callable=AsyncMock) as mock_get, \
         patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock_post:
        
        mock_get.return_value = mock_context_response
        mock_post.return_value = mock_persist_response
        
        # Make multiple requests for the same character
        response1 = client_with_deterministic_seed.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I climb higher"
            }
        )
        
        response2 = client_with_deterministic_seed.post(
            "/turn",
            json={
                "character_id": "550e8400-e29b-41d4-a716-446655440000",
                "user_action": "I climb higher"
            }
        )
        
        # Both should succeed
        assert response1.status_code == 200
        assert response2.status_code == 200
        
        # With deterministic seed, policy decisions should be consistent
        # (though we can't directly verify them from the response)
        # The test mainly verifies no errors occur with deterministic seeding


# =============================================================================
# Multi-Turn End-to-End Tests
# =============================================================================


@pytest.mark.asyncio
async def test_multi_turn_quest_trigger_frequency():
    """Test quest trigger frequency over many turns matches configured probability.
    
    Validates:
    - Quest triggers occur within statistical bounds of configured probability
    - Cooldown enforcement between quest triggers
    - Quest state persists correctly across turns
    """
    from app.services.turn_orchestrator import TurnOrchestrator
    from app.services.policy_engine import PolicyEngine
    from app.services.llm_client import LLMClient
    from app.services.journey_log_client import JourneyLogClient
    from app.prompting.prompt_builder import PromptBuilder
    from app.models import JourneyLogContext, PolicyState, DungeonMasterOutcome, IntentsBlock, QuestIntent, CombatIntent, POIIntent
    from app.services.outcome_parser import ParsedOutcome
    from unittest.mock import AsyncMock, MagicMock
    
    # Configuration
    num_turns = 100
    quest_trigger_prob = 0.3
    quest_cooldown_turns = 5
    rng_seed = 42
    
    # Statistical bounds: With p=0.3 and cooldown=5, the average cycle length is
    # cooldown + 1/p â‰ˆ 5 + 3.33 = 8.33 turns. Over 100 turns, we expect about 12 triggers.
    # Using a 3-sigma interval on a binomial distribution for eligible turns
    # gives a robust range for validation.
    min_expected_triggers = 5
    max_expected_triggers = 21
    
    # Create policy engine with deterministic seed
    policy_engine = PolicyEngine(
        quest_trigger_prob=quest_trigger_prob,
        quest_cooldown_turns=quest_cooldown_turns,
        poi_trigger_prob=0.0,  # Disable POI triggers for this test
        rng_seed=rng_seed
    )
    
    # Mock LLM client
    llm_client = AsyncMock(spec=LLMClient)
    
    # Mock journey log client
    journey_log_client = AsyncMock(spec=JourneyLogClient)
    journey_log_client.persist_narrative = AsyncMock()
    journey_log_client.put_quest = AsyncMock()
    journey_log_client.delete_quest = AsyncMock()
    
    # Create orchestrator
    prompt_builder = PromptBuilder()
    orchestrator = TurnOrchestrator(
        policy_engine=policy_engine,
        llm_client=llm_client,
        journey_log_client=journey_log_client,
        prompt_builder=prompt_builder
    )
    
    # Track quest triggers
    quest_trigger_turns = []
    turns_since_last_quest = 999  # Start high to be eligible
    
    for turn_num in range(num_turns):
        # Create context with updated policy state
        context = JourneyLogContext(
            character_id="test-char-123",
            status="Healthy",
            location={"id": "town", "display_name": "Town"},
            active_quest=None,
            combat_state=None,
            recent_history=[],
            policy_state=PolicyState(
                has_active_quest=False,
                combat_active=False,
                turns_since_last_quest=turns_since_last_quest,
                turns_since_last_poi=999
            )
        )
        
        # Mock LLM response - offer quest if policy triggers
        outcome = DungeonMasterOutcome(
            narrative=f"Turn {turn_num}: You continue your journey.",
            intents=IntentsBlock(
                quest_intent=QuestIntent(action="offer", quest_title="Test Quest", quest_summary="A test quest"),
                combat_intent=CombatIntent(action="none"),
                poi_intent=POIIntent(action="none")
            )
        )
        llm_client.generate_narrative.return_value = ParsedOutcome(
            outcome=outcome,
            narrative=outcome.narrative,
            is_valid=True
        )
        
        # Execute turn
        narrative, intents, summary = await orchestrator.orchestrate_turn(
            character_id="test-char-123",
            user_action=f"I explore (turn {turn_num})",
            context=context,
            user_id=f"test-trace-{turn_num}"
        )
        
        # Check if quest was triggered
        if summary.quest_change.action == "offered" and summary.quest_change.success:
            quest_trigger_turns.append(turn_num)
            turns_since_last_quest = 0
        else:
            turns_since_last_quest += 1
    
    # Validate trigger frequency is within statistical bounds
    num_triggers = len(quest_trigger_turns)
    assert min_expected_triggers <= num_triggers <= max_expected_triggers, (
        f"Quest triggers ({num_triggers}) outside expected range "
        f"[{min_expected_triggers}, {max_expected_triggers}] over {num_turns} turns"
    )
    
    # Validate cooldown enforcement
    if len(quest_trigger_turns) >= 2:
        for i in range(1, len(quest_trigger_turns)):
            turn_gap = quest_trigger_turns[i] - quest_trigger_turns[i-1]
            assert turn_gap > quest_cooldown_turns, (
                f"Cooldown violation: Quest triggered at turns {quest_trigger_turns[i-1]} "
                f"and {quest_trigger_turns[i]} (gap={turn_gap}, cooldown={quest_cooldown_turns})"
            )


@pytest.mark.asyncio
async def test_multi_turn_poi_trigger_frequency():
    """Test POI trigger frequency over many turns matches configured probability.
    
    Validates:
    - POI triggers occur within statistical bounds of configured probability
    - Cooldown enforcement between POI triggers
    - POI state persists correctly across turns
    """
    from app.services.turn_orchestrator import TurnOrchestrator
    from app.services.policy_engine import PolicyEngine
    from app.services.llm_client import LLMClient
    from app.services.journey_log_client import JourneyLogClient
    from app.prompting.prompt_builder import PromptBuilder
    from app.models import JourneyLogContext, PolicyState, DungeonMasterOutcome, IntentsBlock, QuestIntent, CombatIntent, POIIntent
    from app.services.outcome_parser import ParsedOutcome
    from unittest.mock import AsyncMock
    
    # Configuration
    num_turns = 100
    poi_trigger_prob = 0.4
    poi_cooldown_turns = 3
    rng_seed = 123
    
    # Statistical bounds: With p=0.4 and cooldown=3, triggers occur roughly
    # every ~12.5 turns on average, giving ~8 triggers over 100 turns.
    # Using 3-sigma bounds for 99.7% confidence: 5-20 triggers
    # Note: This is an approximation; actual distribution is complex with cooldown interactions
    min_expected_triggers = 5
    max_expected_triggers = 20
    
    # Create policy engine
    policy_engine = PolicyEngine(
        quest_trigger_prob=0.0,  # Disable quest triggers
        poi_trigger_prob=poi_trigger_prob,
        poi_cooldown_turns=poi_cooldown_turns,
        rng_seed=rng_seed
    )
    
    # Mock dependencies
    llm_client = AsyncMock(spec=LLMClient)
    journey_log_client = AsyncMock(spec=JourneyLogClient)
    journey_log_client.persist_narrative = AsyncMock()
    journey_log_client.post_poi = AsyncMock()
    
    # Create orchestrator
    prompt_builder = PromptBuilder()
    orchestrator = TurnOrchestrator(
        policy_engine=policy_engine,
        llm_client=llm_client,
        journey_log_client=journey_log_client,
        prompt_builder=prompt_builder
    )
    
    # Track POI triggers
    poi_trigger_turns = []
    turns_since_last_poi = 999
    
    for turn_num in range(num_turns):
        # Create context
        context = JourneyLogContext(
            character_id="test-char-456",
            status="Healthy",
            location={"id": "forest", "display_name": "Forest"},
            active_quest=None,
            combat_state=None,
            recent_history=[],
            policy_state=PolicyState(
                has_active_quest=False,
                combat_active=False,
                turns_since_last_quest=999,
                turns_since_last_poi=turns_since_last_poi
            )
        )
        
        # Mock LLM response
        outcome = DungeonMasterOutcome(
            narrative=f"Turn {turn_num}: You discover something interesting.",
            intents=IntentsBlock(
                quest_intent=QuestIntent(action="none"),
                combat_intent=CombatIntent(action="none"),
                poi_intent=POIIntent(action="create", name="Test POI", description="A test point of interest")
            )
        )
        llm_client.generate_narrative.return_value = ParsedOutcome(
            outcome=outcome,
            narrative=outcome.narrative,
            is_valid=True
        )
        
        # Execute turn
        narrative, intents, summary = await orchestrator.orchestrate_turn(
            character_id="test-char-456",
            user_action=f"I explore (turn {turn_num})",
            context=context,
            user_id=f"test-trace-{turn_num}"
        )
        
        # Check if POI was triggered
        if summary.poi_change.action == "create" and summary.poi_change.success:
            poi_trigger_turns.append(turn_num)
            turns_since_last_poi = 0
        else:
            turns_since_last_poi += 1
    
    # Validate trigger frequency
    num_triggers = len(poi_trigger_turns)
    assert min_expected_triggers <= num_triggers <= max_expected_triggers, (
        f"POI triggers ({num_triggers}) outside expected range "
        f"[{min_expected_triggers}, {max_expected_triggers}] over {num_turns} turns"
    )
    
    # Validate cooldown enforcement
    if len(poi_trigger_turns) >= 2:
        for i in range(1, len(poi_trigger_turns)):
            turn_gap = poi_trigger_turns[i] - poi_trigger_turns[i-1]
            assert turn_gap > poi_cooldown_turns, (
                f"Cooldown violation: POI triggered at turns {poi_trigger_turns[i-1]} "
                f"and {poi_trigger_turns[i]} (gap={turn_gap}, cooldown={poi_cooldown_turns})"
            )


@pytest.mark.asyncio
async def test_multi_turn_narrative_history_ordering():
    """Test narrative history maintains correct ordering across multiple turns.
    
    Validates:
    - Narrative entries are written in order
    - Recent history is correctly maintained
    - User actions are preserved
    """
    from app.services.turn_orchestrator import TurnOrchestrator
    from app.services.policy_engine import PolicyEngine
    from app.services.llm_client import LLMClient
    from app.services.journey_log_client import JourneyLogClient
    from app.prompting.prompt_builder import PromptBuilder
    from app.models import JourneyLogContext, PolicyState, DungeonMasterOutcome, IntentsBlock, QuestIntent, CombatIntent, POIIntent
    from app.services.outcome_parser import ParsedOutcome
    from unittest.mock import AsyncMock
    
    num_turns = 20
    
    # Create dependencies
    policy_engine = PolicyEngine(
        quest_trigger_prob=0.0,
        poi_trigger_prob=0.0,
        rng_seed=42
    )
    
    llm_client = AsyncMock(spec=LLMClient)
    journey_log_client = AsyncMock(spec=JourneyLogClient)
    journey_log_client.persist_narrative = AsyncMock()
    
    prompt_builder = PromptBuilder()
    orchestrator = TurnOrchestrator(
        policy_engine=policy_engine,
        llm_client=llm_client,
        journey_log_client=journey_log_client,
        prompt_builder=prompt_builder
    )
    
    # Track narrative writes
    narrative_calls = []
    
    def capture_narrative_call(*args, **kwargs):
        narrative_calls.append({
            'character_id': kwargs.get('character_id'),
            'user_action': kwargs.get('user_action'),
            'ai_response': kwargs.get('narrative'),  # Parameter is 'narrative' not 'ai_response'
            'turn_num': len(narrative_calls)
        })
    
    journey_log_client.persist_narrative.side_effect = capture_narrative_call
    
    # Execute multiple turns
    for turn_num in range(num_turns):
        context = JourneyLogContext(
            character_id="test-char-789",
            status="Healthy",
            location={"id": "cave", "display_name": "Cave"},
            active_quest=None,
            combat_state=None,
            recent_history=[],
            policy_state=PolicyState(
                has_active_quest=False,
                combat_active=False,
                turns_since_last_quest=999,
                turns_since_last_poi=999
            )
        )
        
        user_action = f"I take action {turn_num}"
        expected_narrative = f"Turn {turn_num}: The story continues."
        
        outcome = DungeonMasterOutcome(
            narrative=expected_narrative,
            intents=IntentsBlock(
                quest_intent=QuestIntent(action="none"),
                combat_intent=CombatIntent(action="none"),
                poi_intent=POIIntent(action="none")
            )
        )
        llm_client.generate_narrative.return_value = ParsedOutcome(
            outcome=outcome,
            narrative=outcome.narrative,
            is_valid=True
        )
        
        # Execute turn
        await orchestrator.orchestrate_turn(
            character_id="test-char-789",
            user_action=user_action,
            context=context,
            user_id=f"trace-{turn_num}"
        )
    
    # Verify all narratives were written in order
    assert len(narrative_calls) == num_turns
    
    for i, call in enumerate(narrative_calls):
        assert call['turn_num'] == i
        assert call['user_action'] == f"I take action {i}"
        assert f"Turn {i}" in call['ai_response']


@pytest.mark.asyncio
async def test_multi_turn_state_consistency_with_failures():
    """Test state consistency when subsystem writes fail intermittently.
    
    Validates:
    - Narrative always completes even when subsystem writes fail
    - Failed writes are tracked in subsystem_summary
    - Subsequent turns continue normally after failures
    """
    from app.services.turn_orchestrator import TurnOrchestrator
    from app.services.policy_engine import PolicyEngine
    from app.services.llm_client import LLMClient
    from app.services.journey_log_client import JourneyLogClient, JourneyLogClientError
    from app.prompting.prompt_builder import PromptBuilder
    from app.models import JourneyLogContext, PolicyState, DungeonMasterOutcome, IntentsBlock, QuestIntent, CombatIntent, POIIntent
    from app.services.outcome_parser import ParsedOutcome
    from unittest.mock import AsyncMock
    
    num_turns = 10
    
    # Create dependencies
    policy_engine = PolicyEngine(
        quest_trigger_prob=0.5,  # High probability to trigger quests
        poi_trigger_prob=0.5,
        rng_seed=42
    )
    
    llm_client = AsyncMock(spec=LLMClient)
    journey_log_client = AsyncMock(spec=JourneyLogClient)
    
    # Configure narrative to always succeed
    journey_log_client.persist_narrative = AsyncMock()
    
    # Configure quest/POI writes to fail intermittently
    # Track call counts to verify failure pattern is actually triggered
    quest_call_count = [0]
    poi_call_count = [0]
    
    def quest_failure(*args, **kwargs):
        quest_call_count[0] += 1
        if quest_call_count[0] % FAILURE_INTERVAL == 0:  # Fail every 3rd call
            raise JourneyLogClientError("Simulated quest write failure", status_code=500)
    
    def poi_failure(*args, **kwargs):
        poi_call_count[0] += 1
        if poi_call_count[0] % FAILURE_INTERVAL == 0:  # Fail every 3rd call
            raise JourneyLogClientError("Simulated POI write failure", status_code=500)
    
    journey_log_client.put_quest = AsyncMock(side_effect=quest_failure)
    journey_log_client.post_poi = AsyncMock(side_effect=poi_failure)
    
    prompt_builder = PromptBuilder()
    orchestrator = TurnOrchestrator(
        policy_engine=policy_engine,
        llm_client=llm_client,
        journey_log_client=journey_log_client,
        prompt_builder=prompt_builder
    )
    
    # Track successful narrative writes and failures
    successful_narratives = 0
    quest_failures = 0
    poi_failures = 0
    
    # Every 3rd call will fail to simulate intermittent journey-log failures
    FAILURE_INTERVAL = 3
    
    for turn_num in range(num_turns):
        context = JourneyLogContext(
            character_id="test-char-999",
            status="Healthy",
            location={"id": "dungeon", "display_name": "Dungeon"},
            active_quest=None,
            combat_state=None,
            recent_history=[],
            policy_state=PolicyState(
                has_active_quest=False,
                combat_active=False,
                turns_since_last_quest=10,
                turns_since_last_poi=10
            )
        )
        
        outcome = DungeonMasterOutcome(
            narrative=f"Turn {turn_num}: Adventure continues.",
            intents=IntentsBlock(
                quest_intent=QuestIntent(action="offer", quest_title="Quest", quest_summary="Summary"),
                combat_intent=CombatIntent(action="none"),
                poi_intent=POIIntent(action="create", name="POI", description="Description")
            )
        )
        llm_client.generate_narrative.return_value = ParsedOutcome(
            outcome=outcome,
            narrative=outcome.narrative,
            is_valid=True
        )
        
        # Execute turn - should not raise exception
        narrative, intents, summary = await orchestrator.orchestrate_turn(
            character_id="test-char-999",
            user_action=f"Action {turn_num}",
            context=context,
            user_id=f"trace-{turn_num}"
        )
        
        # Verify narrative always completes
        assert narrative is not None
        assert len(narrative) > 0
        successful_narratives += 1
        
        # Track failures in summary
        if not summary.quest_change.success and summary.quest_change.action != "none":
            quest_failures += 1
        if not summary.poi_change.success and summary.poi_change.action != "none":
            poi_failures += 1
    
    # Verify all narratives completed
    assert successful_narratives == num_turns
    
    # Verify failures occurred - at least one of quest or POI should have failures
    # since we're triggering both with high probability over 10 turns
    total_calls = quest_call_count[0] + poi_call_count[0]
    assert total_calls > 0, "Expected at least some quest or POI calls to occur"
    assert quest_failures > 0 or poi_failures > 0, (
        f"Expected some failures to occur. Quest calls: {quest_call_count[0]}, "
        f"POI calls: {poi_call_count[0]}, Quest failures: {quest_failures}, POI failures: {poi_failures}"
    )


@pytest.mark.asyncio
async def test_multi_turn_metrics_capture():
    """Test metrics are captured correctly across multiple turns.
    
    Validates:
    - Turn metrics are recorded for each turn
    - Policy decision metrics are captured
    - Subsystem action metrics are tracked
    """
    from app.services.turn_orchestrator import TurnOrchestrator
    from app.services.policy_engine import PolicyEngine
    from app.services.llm_client import LLMClient
    from app.services.journey_log_client import JourneyLogClient
    from app.prompting.prompt_builder import PromptBuilder
    from app.models import JourneyLogContext, PolicyState, DungeonMasterOutcome, IntentsBlock, QuestIntent, CombatIntent, POIIntent
    from app.services.outcome_parser import ParsedOutcome
    from app.metrics import get_metrics_collector, MetricsCollector
    from unittest.mock import AsyncMock
    
    num_turns = 5
    
    # Create a real metrics collector for testing
    metrics = MetricsCollector()
    
    # Create dependencies
    policy_engine = PolicyEngine(
        quest_trigger_prob=1.0,  # Always trigger for predictable metrics
        poi_trigger_prob=1.0,
        rng_seed=42
    )
    
    llm_client = AsyncMock(spec=LLMClient)
    journey_log_client = AsyncMock(spec=JourneyLogClient)
    journey_log_client.persist_narrative = AsyncMock()
    journey_log_client.put_quest = AsyncMock()
    journey_log_client.post_poi = AsyncMock()
    
    prompt_builder = PromptBuilder()
    orchestrator = TurnOrchestrator(
        policy_engine=policy_engine,
        llm_client=llm_client,
        journey_log_client=journey_log_client,
        prompt_builder=prompt_builder
    )
    
    # Reset metrics before test
    metrics.reset()
    
    for turn_num in range(num_turns):
        context = JourneyLogContext(
            character_id="test-metrics-char",
            status="Healthy",
            location={"id": "test", "display_name": "Test"},
            active_quest=None,
            combat_state=None,
            recent_history=[],
            policy_state=PolicyState(
                has_active_quest=False,
                combat_active=False,
                turns_since_last_quest=10,
                turns_since_last_poi=10
            )
        )
        
        outcome = DungeonMasterOutcome(
            narrative=f"Turn {turn_num}",
            intents=IntentsBlock(
                quest_intent=QuestIntent(action="offer", quest_title="Quest", quest_summary="Summary"),
                combat_intent=CombatIntent(action="none"),
                poi_intent=POIIntent(action="create", name="POI", description="Description")
            )
        )
        llm_client.generate_narrative.return_value = ParsedOutcome(
            outcome=outcome,
            narrative=outcome.narrative,
            is_valid=True
        )
        
        # Execute turn
        await orchestrator.orchestrate_turn(
            character_id="test-metrics-char",
            user_action=f"Action {turn_num}",
            context=context,
            user_id=f"trace-{turn_num}"
        )
    
    # Verify metrics were captured
    # Note: Actual metric validation depends on MetricsCollector implementation
    # This test verifies the test harness exposes hooks to capture metrics
    assert metrics is not None
    
    # Export metrics to verify format
    metrics_output = metrics.get_metrics()
    assert metrics_output is not None
